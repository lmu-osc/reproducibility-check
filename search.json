[
  {
    "objectID": "Manual_Reproducer_Repro-Test.html",
    "href": "Manual_Reproducer_Repro-Test.html",
    "title": "Manual: Reproducibility Test",
    "section": "",
    "text": "Manual: Reproducibility Test\n\nThe Reproducibility Test contains two crucial aspects in total, which determine all steps of this manual and which should always be kept in mind:\n\nA. Reproduction of the analyses: Can all the authors’ figures and results be reproduced with the available data and scripts?\n➔ If this criterion cannot be fulfilled, it does not make sense to continue with B). Therefore, two termination criteria have been built into the manual. Each of them leads to a direct report of the problems/discrepancies to the authors.\nB. Reusability: Are all files (e.g. raw data, scripts) structured in such a way that they can be easily reused by external persons for their own research?\n➔ The document offers some central guiding questions in the second part, which can be used as orientation regarding this aspect. However, you should simply note down everything that could make the reuse of any material/data/scripts difficult.\n\n\nPlease carry out the following steps and note down the respective results regarding the requirements/ questions mentioned within this manual. For this, you can use the associated checklist (REPRO_CHECKLIST_PDF)) that follows the numbering from this manual regarding the requirements.\n\n0. Download all files of the project!\n\nRequirement 0. Are they located in one place? Are they easy to find?\n\n\n1. Code check\na) Execute all analysis scripts in the correct order (e.g. following the instructions from the ReadMe document or the numbering of the documents)\n\nMake sure that the respective programme version (e.g. R version) is used that the authors used for their analyses. The programme version should be specified in the ReadMe document or a separately attached file (e.g. labelled “session_info”). Using the R-installation manager (https://github.com/r-lib/rig) you ca easily install, remove and switch between different R versions within the same R session on MacOs, Windwos and Linux. Other alternatives are Rswitch (https://rud.is/rswitch/) for MacOs or suggestions that can be found at https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop.\nMake sure that the respective software package versions (e.g. R package versions) are used that the authors used for their analyses. To do this, it may be necessary to install older, out-of-date versions of packages that were used when writing the script. Ideally, authors should therefore include a renv file and an R Project file in the project. Then, by calling the R Project file, the whole project can be loaded into R with the correct package versions. Subsequently, you just simply have to execute all the individual scripts in the R project. These then automatically use the versions specified in the renv file, although your current versions can still be used outside the project. However, not all authors follow this optimal procedure. Sometimes the package versions are only specified in the ReadMe document or a separately attached file (e.g. labelled “session_info”). Then, you have to manually load the package versions. In R this is possible using the command install_version(\"psych\", version = \"2.1.6\", repos = \"http://cran.us.r-project.org\"); see https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages for more information. Anyway, the ReadMe file should include instructions for using the correct software versions.\nMake sure to execute all scripts once and in the correct order, especially if a “set.seed” is used at any point in the code. Otherwise, the results can differ considerably. The “set.seed” ensures that the same random values are selected in random processes (see also: https://r-coder.com/set-seed-r/).If a “set.seed” command is used, this should also be noted in the ReadMe file, e.g. in the description of all necessary steps for the complete reproduction of the analyses.\n\n\nRequirement 1.1. Do all analyses run without errors or are there any error messages/omissions? Make a note of the errors and the scripts which produced them! How long does it take to run through all the scripts? Does the time roughly correspond to the authors’ specifications in the ReadMe document (if available)? Make a note of the time and report back!\n\n➔ 1st Termination criterion (➔ direct feedback to authors):\nIf the scripts cannot be run through, first look for a solution to the problem yourself (e.g. with the help of the ReadMe file or a quick google search). Note down successful solutions in order to be able to report them back to the authors later as a tip/hint. If no solution is found: Please report the problem directly to the authors and interrupt the reproducibility test at this point until a solution is found!\n\nb) Do the actual check by comparing the reproduced results with the authors’ results! Using the scripts or the README-file it should be easy to identify which tables, figures and in-text numbers are produced by which script. List the comparisons in an Excel spreadsheet according to the following scheme (see Table below) and describe any differences:\n\nIn the results section of the paper, mark all passages (including tables/figures/results within the body text) that contain results (especially figures, graphs) of the analyses carried out. Compare these individually with your own results from the executed scripts. The numbers within the body text do not have to be added individually to the spreadsheet; it is sufficient if all sections within the Results are listed with their headings. The authors should make clear where the respective results can be found (e.g. within ReadMe document and/or the scripts by adding appropriate comments).\nIf there are data sets in the project folder that were created by other pre-processing scripts (e.g. in folders called “Processed Data”), these data sets should also be compared with the data sets that were created during the reproduction test. For this purpose, there are so-called dif tools that can be used to look for differences between two files (e.g. csv). This way you don’t have to go through every single line yourself, which can be very time-consuming with long data sets. A recommendation of different dif tool programmes can be found at https://www.git-tower.com/blog/diff-tools-windows (for Windows) or https://www.git-tower.com/blog/diff-tools-mac/ (for Mac). If you find differences in these generated data sets and report them, it makes it easier for the authors to find out where the problems occurred during the reproduction attempt.\n\n\n\n\n\n\n\n\n\n\nPreprocessing script\nData sets created\n\nSuccessfully reproduced? If not, list all differences here!\n\n\n\n\ne.g. 0a preprocessing\ne.g. processed_data.csv\n\ne.g. Yes\n\n\n…\n…\n\n…\n\n\nGraphic/table/number in text\nScript which produced the respective output\nLine in the manuscript\nSuccessfully reproduced? If not, list all differences here!\n\n\ne.g. Figure 1\ne.g. 2-Analyses\nAfter line 300\ne.g. different size of the last bar (chart)\n\n\ne.g. Descriptive statistics of the participants\ne.g. 1-Demographics\ne.g. line 320\ne.g. Yes\n\n\n…\n…\n…\n…\n\n\n\n➔ 2nd Termination criterion (➔ direct feedback to authors):\nIf any results cannot be reproduced, first very briefly check possible simple sources of error yourself (e.g. wrong sequence when executing the scripts; different software versions). If no solution is found: Please report back directly to the authors (➔send completed excel spreadsheet) and interrupt the repro test at this point until a solution is found!\n\n\nRequirement 1.2. Are all tables from the manuscript reproducible? Make a note and SAVE file (screen-shot/jpg) if any table looks different!\n\n\nRequirement 1.3. Are all figures from the manuscript reproducible? Make a note and SAVE file (screen-shot/jpg) if any figure looks different!\n\n\nRequirement 1.4. All all in-text-numbers from the manuscript reproducible? Make a note of the numbers if any in-text-number is different!\n\n\nRequirement 1.5. Is it possible and easy to identify which tables, figures and in-text numbers are produced by which script/program (e.g. using the README-document and/or the comments within the scripts)?\n\nc) Go through individual analysis scripts (e.g. R scripts)!\n\nRequirement 1.6. Are the files named in a meaningful way, so that one can immediately derive from the title of each script what it is about and in which order the scripts should be executed?\n\n\nRequirement 1.7. Does a “Master” script file exist that can be executed to automatically run all other scripts without having to run them individually yourself?\n\n\nRequirement 1.8. Are the lines of code sufficiently commented so that one can basically understand which steps are carried out?\n\n\n2. Search for ReadMe file!\n\nRequirement 2.1. Does it provide brief meta-information about the project (e.g. author, name of the paper, approximate topic)?\n\n\nRequirement 2.2. Does it include an overview of all files (data, scripts) including their source and availability (e.g. name of specific folders)?\n\n\nRequirement 2.3. Does it describe all the necessary steps to fully reproduce the analyses?\n\n\nRequirement 2.4. Does it sufficiently point out requirements/prerequisites regarding the software to be used (e.g. versions of the R packages; running time of the scripts; working memory of the computer)?\n\n\n3. Get an overview of all research data!\n\nRequirement 3.1. Are there general descriptions/meta-data for all raw/primary data that give general information about their content (e.g. survey periods, survey locations, type of data)? These should allow potential re-users to quickly decide whether the data is useful for their purposes. This meta-data should be easily accessible via the link to the data in the paper (e.g. located directly on the website where the data is stored). Ideally, an additional meta file in json format should be included, following the recommendations within this google-document https://docs.google.com/document/d/1u8o5jnWk0Iqp_J06PTu5NjBfVsdoPbBhstht6W0fFp0/edit#heading=h.v795m5ev9q and the section there entitled “6.1. Dataset-level metadata (dataset_description.json)”.\n\n\nRequirement 3.2. Is there a direct link to the data in the paper?\n\n\nRequirement 3.3. Do all raw/primary data have codebooks that describe all variables contained in the files in an understandable way? This does not apply to the data that is only created with the specified scripts (e.g. data in folders with names like “processed_data”), but only to the data that is actually necessary to run the scripts in the first place.\n\n\nRequirement 3.4. Are there summary statistics (e.g. mean values/standard deviations) for all raw/primary data?\n\n\nRequirement 3.5. Is there a data citation provided somewhere? It should be provided within the paper if external data are used or within ReadMe/paper/project folder to reuse data if they were produced within the same project.\n\n\nRequirement 3.6. Do all files have a CSV-, TSV- or TAB-format (necessary to ensure compatibility with various programmes) and can they be opened without any problems?\n\n\nRequirement 3.7. Are all variables in the data sets named?\n\n\nRequirement 3.8. Are all data sufficiently anonymised (especially anonymisation/removal of personal details such as names/birth dates)? This point does not need to be checked specifically by independently searching for non-anonymised data. However, it is important to note and pass on any anomalies encountered when conducting the reproducibility test.\n\nThink of any additional aspects that could be pointed out to the authors, even if they are not mandatory. Examples are:\n\nAre the data stored in a data archive? This makes the data easier to be found and thus easier to reuse.\nAre there any other parts of the workflow that authors could improve (e.g. shorten code)?\nAre the statistical methods used suitable for answering the research question?\nWould it be reasonable to provide a “toy” example, or smaller dataset that can be quickly analysed to demonstrate that the workflow runs correctly?\n…"
  },
  {
    "objectID": "peer-reproduction.html",
    "href": "peer-reproduction.html",
    "title": "Peer Reproduction",
    "section": "",
    "text": "Peer reproductions provide the possibility for research groups to check the reproducibility of their works by leveraging the existing knowledge withing a group. They are convenient if sharing data with third parties is difficult given privacy concerns and contractual obligations.\nWe provide a web tool that guides researchers through the process of conducting a reproduction. Using the tool guarantees that a common protocol is followed and important information is always collected. It supports the user by generating a report in the end facilitating the communication about the reproduction."
  },
  {
    "objectID": "peer-reproduction.html#using-the-web-tool",
    "href": "peer-reproduction.html#using-the-web-tool",
    "title": "Peer Reproduction",
    "section": "Using the web tool",
    "text": "Using the web tool\nOne can start a new reproduction by clicking on the following button:\nOpen web tool\nThe web tool will open in a new tab and guide you through the process. It roughly follows the structure suggested by Kohrt et al. (2024).\nAs of May 2025, the web tool is in beta status. This means that it can already be used for its purpose, but likely contains bugs and unpolished interfaces.\nPlease feel free to report issues at GitHub."
  },
  {
    "objectID": "Manual_Researchers_Repro-Test.html",
    "href": "Manual_Researchers_Repro-Test.html",
    "title": "Manual Researchers: Reproducibility Test",
    "section": "",
    "text": "Manual Researchers: Reproducibility Test\n\nIf you want to guarantee the reusability of your data and code, and a successful reproducibility attempt by the reproducer, you should keep in mind the following when finalizing your data, script and other project files.\n\nThe Reproducibility Test generally contains these two aspects:\n\nA. Reproduction of the analyses: Can all your figures and results be reproduced with the available data and scripts?\n➔ If this criterion cannot be fulfilled, it does not make sense to continue with B). Therefore, two termination criteria (1. scripts cannot be run through; 2. Figures/results cannot be reproduced) have been built into the manual for the reproducer. Each of them leads to a direct report of the problems/discrepancies to you as the author.\nB. Reusability: Are all files (e.g. raw data, scripts) structured in such a way that they can be easily reused by external persons for their own research?\n➔ You should think about everything that could make the reuse of your material/data/scripts difficult. Three central aspects to improve the reusability are a helpful code description (e.g. understandable comments within your scripts), reasonable and consistent variable names within both data files and codebooks and some kind of meta-data that provide a quick overview what your data and project are about.\n\n\nIt is important to provide all necessary documents and information so that the reproducer can at least complete a full Reproducibility Test. The following manual aims to provide you with some central steps to a) make sure that the reproducibility test can be conducted completely and b) guarantee some minimal standards which will help to make the reproducibility attempt as successful as possible.\n\n\nUpload all files of your project into one place (e.g. an osf-project) or at least make sure that all information how all files are accessible can be found at one place. The project and its link should be a persistent archive that will not be removed or change its address.\n0.1. Make sure that files containing your (raw) data have a CSV-, TSV- or TAB-format (necessary to ensure compatibility with various programmes). 0.2. Use UTF-8 (or UTF-16) encoding for all files to avoid problems in an international context (e.g., so characters like ü or é aren’t mangled) 0.3. Make sure to include codebooks for at least all raw data files.\n0.4. Optimally, your project contains a R project file and such a structure of the folders and data files that the reproducer can simply download and save the complete project folder and then execute the scripts without further changes. Therefore, the scripts should use relative paths for loading data/ other scripts (e.g. “…/data/raw_data”) 0.5. Create a “Master” script file (e.g. named 0_Make.R) that can be executed by the reproducer to automatically run all other scripts without having to run them individually. 0.6. You should use a “set.seed” (https://r-coder.com/set-seed-r/) in your scripts if you simulate something or use a random number generator to guarantee that the same “random numbers” will be used. 0.7. Make sure that the project includes a ReadMe-file (see 2. for what should be included in this file). 0.8. Ethical sharing: No uploaded data should contain any information that makes participants identifiable. 0.9. Add individual citation info for (raw) data and analysis script(s), so that people can use it to cite your work when reusing it. Optimally, you add a DOI for code and data that is also included in this citation info. This makes your code and data findable.\n\nMake sure that the correct versions of the software and software packages (e.g. versions of R packages) can be used by the reproducer (dependency management). Therefore, you might choose between the following options ranked by their quality (best option first):\n\nProvide a completely reproducible workflow with R Markdown, Git, a “Make-file”, and containers like Docker (https://www.docker.com/products/docker-desktop/) or CodeOceanHQ (https://codeocean.com/). This automatically guarantees that the analysis will be reproduced in the exact same environment that you used to conduct them. A more concrete suggestion on how to implement such a workflow can be found in a paper by Peikert & Bramdmaier(2021, https://www.imprs-life.mpg.de/318048/12_peikert_brandmaier_21.pdf).\nTo guarantee a kind of minimal dependency management, so that the author can at least automatically use the correct package versions: Include a renv file that specifies the used package versions and an R Project file in the project. More information about this option can be found at https://www.rstudio.com/blog/renv-project-environments-for-r/. Another option among others for R would be packrat (see https://rstudio.github.io/packrat/).\nInclude an additional file where all package versions are specified. Therefore, you can include the output of the code “devtools::session_info()” from the R session where you have produced your results or “requirements.txt” in python.\n\n\n\n\nAdd a ReadMe-file (in a TXT, MD or PDF-format) to the project that explains all steps necessary to reproduce your analyses, especially the following points:\n2.1. Programme version (e.g. R-version) that you used or that is necessary to reproduce your analyses.\n2.2. Information about requirements regarding the computational environment (e.g. working memory of the computer, Mac/Windows version of the computer, approximate running time of your code)\n2.3. Information about how to make sure that the correct versions of the software packages (e.g. versions of R packages) can be used by the reproducer. Therefore, you might choose between the options stated in point 1.\n2.4. Information on where to find all (raw) data files necessary to reproduce your analyses. Here, you can also shortly add a short note if any data files included in your project are no raw data but instead should be reproduced by the reproducer himself/herself (e.g. by running the pre-processing scripts using raw data files)\n2.5. Information on how to save the data files in order to be able to successfully execute the scripts. Optimally, your project contains a R project file and such a structure of the folders and data files that the reproducer can simply download and save the complete project folder and then execute the scripts without further changes.\n2.6. Information on the order in which the R scripts should be executed to reproduce your results. If you use a “set.seed” ((see also: https://r-coder.com/set-seed-r/), it would be a good idea to add an additional comment about it within the ReadMe-file, so that the reproducer does not run into any error e.g. repeatedly executing parts of this script. Here, you can also mention your “Master” script file (e.g. named 0_Make.R) if you included one. 2.7. Indication of which parts of the code produce which figures/tables/numbers. Alternatively, you can also use RMarkdown so that the whole paper will be reproduced.\n\n\n\nAdd a LICENCE.txt file that defines the license under which the code can be reused. Typical choices are MIT (a permissive license) or GPLv3 (a copyleft license). See https://choosealicense.com or https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002598 for guidance on how to choose an appropriate license.\n\n\n\nSelf-check the reproduction of your analyses by downloading data and code from the created project and follow the instructions that you give in the ReadMe file. Optimally, you do that on your own and another computer (e.g. your colleague’s).\n\n\n\nWrite an e-mail to the reproducer including…\n4.1. …the paper whose analyses should be reproduced\n4.2. …a link to your project or an information about where to find all documents necessary to reproduce your analyses\n4.3. …a short reference to the ReadMe-file telling the reproducer that it should contain all instructions to reproduce your analyses"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducibility Check",
    "section": "",
    "text": "Welcome to the Reproducibility Check project at the LMU Open Science Center! Here, we strive to increase reproducibility of scientific works by providing guidance and services to researchers."
  },
  {
    "objectID": "index.html#what-is-reproducibility",
    "href": "index.html#what-is-reproducibility",
    "title": "Reproducibility Check",
    "section": "What is reproducibility?",
    "text": "What is reproducibility?\nBy reproducibility, we mean…\n\n“obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis”\n— National Academies of Sciences, Engineering, and Medicine (2019), p. 46"
  },
  {
    "objectID": "index.html#what-do-we-offer",
    "href": "index.html#what-do-we-offer",
    "title": "Reproducibility Check",
    "section": "What do we offer?",
    "text": "What do we offer?\nCurrently, we provide guidance via a web tool for research groups conducting peer reproductions.\nPeer Reproduction\nIn the future, we also want to provide reproducibility checks as a service."
  },
  {
    "objectID": "index.html#who-is-behind-this",
    "href": "index.html#who-is-behind-this",
    "title": "Reproducibility Check",
    "section": "Who is behind this?",
    "text": "Who is behind this?\nThis project is coordinated by the LMU Open Science Center. The initial development was done by Florian Kohrt."
  }
]